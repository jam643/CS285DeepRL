{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2 Policy Gradient\n",
    "## Experiment 1: CartPole with PG\n",
    "Example sim result:\n",
    "\n",
    "| iter 0 | iter 90 |\n",
    "| :---: | :---: |\n",
    "| <img src=\"cartpole_iter0.gif\" width=\"200\"/> | <img src=\"cartpole.gif\" width=\"270\"/> |\n",
    "\n",
    "**Learning Curves (Avg Return):**\n",
    "\n",
    "| Small Batch (1000) | Large Batch (5000) |\n",
    "| :---: | :---: |\n",
    "| <img src=\"sb_cartpole.png\" width=\"600\"/> | <img src=\"lb_cartpole.png\" width=\"600\"/> |\n",
    "\n",
    "* The reward-to-go estimater outperformed the trajectory-centric estimator in both cases. This makes sense as it takes advantage of causality (later actions cannot affect previous rewards).\n",
    "* Advantage standardization also helps (mostly for small batch) as it acts as a baseline, reducing the variance of monte-carlo returns.\n",
    "* Larger batch size improved performance as it makes the monte-carlo estimates of reward-to-go more accurate. As batch size goes to infinity, monte-carlo estimate converges to true expected reward to go, as it is unbiased.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Inverted Pendulum\n",
    "\n",
    "Able to achieve 1000 frame episode length with `batch = 100` an `learning rate = 0.01`\n",
    "\n",
    "<img src=\"invertedpendulum_returns.png\" width=\"800\"/>\n",
    "\n",
    "command:\n",
    "\n",
    "```python cs285/scripts/run_hw2.py -ngpu --video_log_freq 10 --env_name InvertedPendulum-v2 --ep_len 1000 --discount 0.9 -n 100 -l 2 -s 64 -b 200 -lr 0.01 -rtg --exp_name q2_b200_r0.01```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3: Lunar Lander - NN Baseline\n",
    "Example sim eval result. Note that for each iterations:\n",
    "* 40,000 training samples are collected with current policy.\n",
    "* This corresponds to a min of 40 rollouts (1000 episode length max, or more if e.g. lander crashes early)\n",
    "* Reward-to-go Q-values (via discounted cumsum) are assigned to trajectories\n",
    "* A single step (learning rate 0.005) is taken in direction of approximated RL objective (fn of policy)\n",
    "\n",
    "\n",
    "| iter 0 | iter 90 |\n",
    "| :---: | :---: |\n",
    "| <img src=\"lunarlander_iter0.gif\" width=\"400\"/> | <img src=\"lunarlander.gif\" width=\"400\"/> |\n",
    "\n",
    "**Lunar Lander Avg Return:**\n",
    "Blue shows return with neural net state-dependent, baseline (value function), red is without baseline. Baseline shows improved performance as it reduces variance.\n",
    "<img src=\"lunarlandar_return.png\" width=\"800\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 4: Half Cheetah\n",
    "I was lazy and didn't search over batch sizes and learning rates (just chose `lr=0.02` `batch=50000`). Results below show:\n",
    "* reward-to-go (blue) has the biggest affect\n",
    "* Using value fn baseline shows slight improvement with reward-to-go but no distinguishable difference otherwise.\n",
    "<img src=\"halfCheetah_returns.png\" width=\"800\"/>\n",
    "\n",
    "Interestingly, the rtg that recieved higher return found an undesireable solution where the cheetah flips onto it's back and then flails around to make progress:\n",
    "\n",
    "| traj based | reward-to-go |\n",
    "| :---: | :---: |\n",
    "| <img src=\"halfCheetah.gif\" width=\"300\"/> | <img src=\"halfCheetah_rtg.gif\" width=\"300\"/> |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 5: Hopper - GAE\n",
    "Avg returns shown below. Results suggest:\n",
    "* $\\lambda=0.0$ (one step Value fn backup) performed worse by far, possibly due to high bias that is added.\n",
    "* $\\lambda=0.95$ and $\\lambda=0.99$ performed best (balance bias with variance)\n",
    "* $\\lambda=1.0$ performed nearly as good, and actually had the best returns by the end\n",
    "* The videos below do show the best looking policy to be $\\lambda=0.95$, where the hopper performs multiple hops, vs $\\lambda=1.0$, where it only performs a single long-jump hop.\n",
    "\n",
    "<img src=\"hopper_returns.png\" width=\"800\"/>\n",
    "\n",
    "\n",
    "| $\\lambda=0.95$ | $\\lambda=1.0$ |\n",
    "| :---: | :---: |\n",
    "| <img src=\"hopper_lambdap95.gif\" width=\"400\"/> | <img src=\"hopper_lambda1.gif\" width=\"400\"/> |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs285",
   "language": "python",
   "name": "cs285"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
