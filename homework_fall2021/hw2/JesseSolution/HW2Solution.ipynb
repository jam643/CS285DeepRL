{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2 Policy Gradient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: CartPole with PG\n",
    "Example sim result:\n",
    "\n",
    "| iter 0 | iter 90 |\n",
    "| :---: | :---: |\n",
    "| <img src=\"cartpole_iter0.gif\" width=\"200\"/> | <img src=\"cartpole.gif\" width=\"270\"/> |\n",
    "\n",
    "**Learning Curves (Avg Return):**\n",
    "\n",
    "| Small Batch (1000) | Large Batch (5000) |\n",
    "| :---: | :---: |\n",
    "| <img src=\"sb_cartpole.png\" width=\"600\"/> | <img src=\"lb_cartpole.png\" width=\"600\"/> |\n",
    "\n",
    "* The reward-to-go estimater outperformed the trajectory-centric estimator in both cases. This makes sense as it takes advantage of causality (later actions cannot affect previous rewards).\n",
    "* Advantage standardization also helps (mostly for small batch) as it acts as a baseline, reducing the variance of monte-carlo returns.\n",
    "* Larger batch size improved performance as it makes the monte-carlo estimates of reward-to-go more accurate. As batch size goes to infinity, monte-carlo estimate converges to true expected reward to go, as it is unbiased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Inverted Pendulum\n",
    "\n",
    "Able to achieve 1000 frame episode length with `batch = 100` an `learning rate = 0.01`\n",
    "\n",
    "<img src=\"invertedpendulum_returns.png\" width=\"800\"/>\n",
    "\n",
    "command:\n",
    "\n",
    "```python cs285/scripts/run_hw2.py -ngpu --video_log_freq 10 --env_name InvertedPendulum-v2 --ep_len 1000 --discount 0.9 -n 100 -l 2 -s 64 -b 200 -lr 0.01 -rtg --exp_name q2_b200_r0.01```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3: Lunar Lander - NN Baseline\n",
    "Example sim eval result. Note that for each iterations:\n",
    "* 40,000 training samples are collected with current policy.\n",
    "* This corresponds to a min of 40 rollouts (1000 episode length max, or more if e.g. lander crashes early)\n",
    "* Reward-to-go Q-values (via discounted cumsum) are assigned to trajectories\n",
    "* A single step (learning rate 0.005) is taken in direction of approximated RL objective (fn of policy)\n",
    "\n",
    "\n",
    "| iter 0 | iter 90 |\n",
    "| :---: | :---: |\n",
    "| <img src=\"lunarlander_iter0.gif\" width=\"400\"/> | <img src=\"lunarlander.gif\" width=\"400\"/> |\n",
    "\n",
    "**Lunar Lander Avg Return:**\n",
    "Blue shows return with neural net state-dependent, baseline (value function), red is without baseline. Baseline shows improved performance as it reduces variance.\n",
    "<img src=\"lunarlandar_return.png\" width=\"800\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 4: Half Cheetah\n",
    "I was lazy and didn't search over batch sizes and learning rates (just chose `lr=0.02` `batch=50000`). Results below show:\n",
    "* reward-to-go (blue) has the biggest affect\n",
    "* Using value fn baseline shows slight improvement with reward-to-go but no distinguishable difference otherwise.\n",
    "<img src=\"halfCheetah_returns.png\" width=\"800\"/>\n",
    "\n",
    "Interestingly, the rtg that recieved higher return found an undesireable solution where the cheetah flips onto it's back and then flails around to make progress:\n",
    "\n",
    "| traj based | reward-to-go |\n",
    "| :---: | :---: |\n",
    "| <img src=\"halfCheetah.gif\" width=\"300\"/> | <img src=\"halfCheetah_rtg.gif\" width=\"300\"/> |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 5: Hopper - GAE\n",
    "Avg returns shown below. Results suggest:\n",
    "* $\\lambda=0.0$ (one step Value fn backup) performed worse by far, possibly due to high bias that is added.\n",
    "* $\\lambda=0.95$ and $\\lambda=0.99$ performed best (balance bias with variance)\n",
    "* $\\lambda=1.0$ performed nearly as good, and actually had the best returns by the end\n",
    "* The videos below do show the best looking policy to be $\\lambda=0.95$, where the hopper performs multiple hops, vs $\\lambda=1.0$, where it only performs a single long-jump hop.\n",
    "\n",
    "<img src=\"hopper_returns.png\" width=\"800\"/>\n",
    "\n",
    "\n",
    "| $\\lambda=0.95$ | $\\lambda=1.0$ |\n",
    "| :---: | :---: |\n",
    "| <img src=\"hopper_lambdap95.gif\" width=\"400\"/> | <img src=\"hopper_lambda1.gif\" width=\"400\"/> |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PG Pseudo-code\n",
    "\n",
    "Example from Q5\n",
    "\n",
    "---\n",
    "**for** n_iter (e.g. 300)\n",
    "<blockquote>\n",
    "    collect_training_trajectories with current policy\n",
    "    <blockquote>\n",
    "        - with batch_size (2k) env samples with ep_len (1k) samples per rollout (i.e. 2 rollouts) <br>\n",
    "        - sampling actions from $\\pi(a|o)$ distribution\n",
    "    </blockquote>    \n",
    "    for num_agent_train_steps_per_iter (e.g. 1):   \n",
    "    <blockquote>\n",
    "        - sample train_batch_size (2k) most recent samples (need to be collected from curr policy due to on-policy)<br>\n",
    "        - compute Monte-Carlo return estimator Q-vals<br>\n",
    "        if reward-to go:\n",
    "        <blockquote>\n",
    "            rtg decreasing over rollout: $r_{i}=\\sum_{t'=t}^{T-1} {\\gamma^{t'-t}r(s_{it'},a_{it'}})$\n",
    "        </blockquote>\n",
    "        else:\n",
    "        <blockquote>\n",
    "            return est is const over rollout: $r_{i}=\\sum_{t'=0}^{T-1} {\\gamma^{t'}r(s_{it'},a_{it'}})$\n",
    "        </blockquote>\n",
    "        - Estimate advantage from return_est minus baseline $(Q_t - b_t)$. E.g. NN value fn estimator $b_t=V_{NN}(obs_t)$.<br>\n",
    "        - Perform one grad step in policy with $\\sum_{t=0}^{T-1} [\\nabla [log \\pi(a_t|o_t) * (Q_t - b_t)]]$<br>\n",
    "        So `loss=-Sum(log_prob_of_action * advantages)` where gradient is not propagated through baseline NN used for advantage estimation<br>\n",
    "        - Perform one grad step in NN baseline (value fn estimator, normalized)\n",
    "    </blockquote>\n",
    "</blockquote>\n",
    "\n",
    "**end for**\n",
    "\n",
    "---\n",
    "where `MLPPolicyPG` has 2 hidden layers with `tanh` activation and 32 nodes each. With outputs:\n",
    "- discrete: categorical distro, e.g. `[0.2, 0.7, 0.1]` would repr prob for actions: `[a0, a1, a2]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs285",
   "language": "python",
   "name": "cs285"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
